<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Basics</title>
<meta name="generator" content="Org mode">
<meta name="author" content="George Kontsevich">
<meta name="description" content="Basics in Stats using Octave/MATLAB"
>
<link rel="stylesheet" type="text/css" href="../web/worg.css" />
<link rel="shortcut icon" href="../web/panda.svg" type="image/x-icon">
</head>
<body>
<div id="content">
<h1 class="title">Basics</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgfb8b9ac">Intro</a></li>
<li><a href="#org55f8196">Continious random variables</a></li>
<li><a href="#org1bcef87">Probability density functions (PDFs)</a>
<ul>
<li><a href="#org6b0ae55">Example: Normal distribution</a></li>
<li><a href="#org5840b5c">Probability densities</a></li>
<li><a href="#org43721ef">Estimating a PDF</a></li>
<li><a href="#orgac9d27a">Example: Estimating the Normal</a></li>
</ul>
</li>
<li><a href="#org7d7559f">Descriptive statistics</a>
<ul>
<li>
<ul>
<li><a href="#org31ae7ab">Expected Value/Mean</a></li>
<li><a href="#org10d4117">Variance</a></li>
<li><a href="#org6a4b6f3">Standard deviation</a></li>
</ul>
</li>
<li><a href="#org25cc05f">Estimating the Normal's descriptive statistics</a>
<ul>
<li><a href="#orgdb2cd25">Mean of the Normal</a></li>
<li><a href="#org4caa3d3">Standard deviation of the normal</a></li>
</ul>
</li>
<li><a href="#org30705d3">SigFigs</a></li>
<li><a href="#org2f628aa">Adding two random variables</a>
<ul>
<li><a href="#org6f16601">Expected Value</a></li>
<li><a href="#orgf15435c">Variance</a></li>
<li><a href="#org282a577">Standard Deviation</a></li>
<li><a href="#org0e47a72">SigFigs</a></li>
<li><a href="#org3105f5d">Example: Standard deviation of the mean (SDOM)</a></li>
</ul>
</li>
<li><a href="#orgdd250d7">Multiplying two random variables</a>
<ul>
<li><a href="#org0c072b0">Expected Value</a></li>
<li><a href="#orgb05d64a">Variance</a></li>
<li><a href="#orgf1428ec">Standard Deviation</a></li>
<li><a href="#orgd099f51">SigFigs</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgc3eb256">Other Topics</a>
<ul>
<li><a href="#org21f9090">Data Rejection</a>
<ul>
<li><a href="#org433077b">Chauvenet's Criterion</a></li>
</ul>
</li>
<li><a href="#org0a3c2f0">Weighted Average</a>
<ul>
<li><a href="#org41b101c">Mean</a></li>
<li><a href="#orgb169a8e">Standard Deviation</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org8baf549">Multiple Variables</a>
<ul>
<li><a href="#org48f8e88">Functions of two variables</a>
<ul>
<li><a href="#orge74a6c5">Mean</a></li>
<li><a href="#orgfb0891e">Variance</a></li>
</ul>
</li>
<li><a href="#org0931e6f">Correlation Coefficient</a></li>
</ul>
</li>
<li><a href="#org8a35618">End</a></li>
</ul>
</div>
</div>

<div id="outline-container-orgfb8b9ac" class="outline-2">
<h2 id="orgfb8b9ac">Intro</h2>
<div class="outline-text-2" id="text-orgfb8b9ac">
<p>
A digest of basic statistics from reading John Taylor's <i>An Introduction to Error Analysis: The Study of Uncertainties in Physical Measurements</i>
</p>

<p>
The book generally works "backwards": Starting from useful principles of working with uncertain values and then working through the justification and proofs. Since this is a quick digest and personal referesher/reference, the order is opposite of that presented in the book. When it makes sense I had inserted some MATLAB/Octave code to demonstrate things
</p>
</div>
</div>
<div id="outline-container-org55f8196" class="outline-2">
<h2 id="org55f8196">Continious random variables</h2>
<div class="outline-text-2" id="text-org55f8196">
<p>
Are variables that when measured give a value of infinite precision. So no number can be measured twice
</p>
</div>
</div>

<div id="outline-container-org1bcef87" class="outline-2">
<h2 id="org1bcef87">Probability density functions (PDFs)</h2>
<div class="outline-text-2" id="text-org1bcef87">
<p>
Probability density functions (PDFs) are the basic functions that describe a continious random variable. The basic property of a PDF is that it can be integrated between any two value. The resulting value is the probability that your measurement will appear between the two values. Since each measurements yields <i>some</i> value, integrate between -&infin; and &infin; must be equal to 1 (ie. 100%). The condition places a constraint on which functions are valid PDFs.
</p>

<div class="org-src-container">
<pre class="src src-octave">plot_range <span style="color: #483d8b;">=</span> 5
sigma <span style="color: #483d8b;">=</span> 1
center <span style="color: #483d8b;">=</span> 3
tau <span style="color: #483d8b;">=</span> 3.5
t <span style="color: #483d8b;">=</span> [<span style="color: #483d8b;">-</span>10<span style="color: #483d8b;">*</span>plot_range<span style="color: #483d8b;">:</span>plot_range<span style="color: #483d8b;">/</span>100<span style="color: #483d8b;">:</span>plot_range]
plot(t<span style="color: #483d8b;">,</span>(1<span style="color: #483d8b;">/</span>tau)<span style="color: #483d8b;">*</span>exp(<span style="color: #483d8b;">-</span>t<span style="color: #483d8b;">/</span>tau))
<span style="color: #b22222;">%</span><span style="color: #b22222;">axis "off"</span>
hold on<span style="color: #483d8b;">;</span>
<span style="color: #b22222;">%</span><span style="color: #b22222;">integration_xs = [a:(plot_range/50):b]</span>
<span style="color: #b22222;">%</span><span style="color: #b22222;">area(integration_xs,normal_dist(integration_xs,center,sigma), "facecolor",[0.74 0.9 1.0])</span>
print <span style="color: #8b2252;">"-S720,160"</span> <span style="color: #8b2252;">"exponential.svg"</span>
hold off<span style="color: #483d8b;">;</span>
</pre>
</div>

<div class="figure">
<p><object type="image/svg+xml" data="./exponential.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
</div>
</div>

<div id="outline-container-org6b0ae55" class="outline-3">
<h3 id="org6b0ae55">Example: Normal distribution</h3>
<div class="outline-text-3" id="text-org6b0ae55">
<p>
The most common PDF is the <i>normal distribution</i> and it'll serve as a frequent example
</p>

<blockquote>
<p>
[1/(&sigma;&radic;2&pi;)] * e<sup>([x-X<sub>center</sub>]<sup>2</sup>)/(2&sigma;<sup>2</sup>)</sup>
</p>
</blockquote>

<p>
It has two parameters, <b>&sigma;</b> and <b>X<sub>center</sub></b>. Other functions may have different parameters. As we will see later, in this case these two happen to correspond to the <i>mean</i> and <i>standard deviation</i>, but this won't be the case for all PDFs.
</p>

<div class="org-src-container">
<pre class="src src-octave"><span style="color: #a020f0;">function</span> [points] <span style="color: #483d8b;">=</span> <span style="color: #0000ff;">normal_dist</span>(xs<span style="color: #483d8b;">,</span> center<span style="color: #483d8b;">,</span> sigma)
  normalization_factor <span style="color: #483d8b;">=</span> 1<span style="color: #483d8b;">/</span>(sigma<span style="color: #483d8b;">*</span>sqrt(2<span style="color: #483d8b;">*</span>pi))
  points <span style="color: #483d8b;">=</span> normalization_factor <span style="color: #483d8b;">*</span> e<span style="color: #483d8b;">.^</span>(<span style="color: #483d8b;">-</span>((xs<span style="color: #483d8b;">-</span>center)<span style="color: #483d8b;">.^</span>2)<span style="color: #483d8b;">/</span>(2<span style="color: #483d8b;">*</span>sigma<span style="color: #483d8b;">^</span>2))
<span style="color: #a020f0;">end</span>
</pre>
</div>

<p>
We can plot it to see how it looks
</p>


<div class="figure">
<p><object type="image/svg+xml" data="./normal.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
</div>
</div>
</div>

<div id="outline-container-org5840b5c" class="outline-3">
<h3 id="org5840b5c">Probability densities</h3>
<div class="outline-text-3" id="text-org5840b5c">
<p>
Again, because the random variable is continious, no value can be measured twice. We can see this in the PDF as well. If we integrate around some point x<sub>0</sub>. The integral from <i>x<sub>0</sub></i> to <i>x<sub>0</sub>+&delta;x</i> will go to zero as <i>&delta;x</i> goes to zero. We know that for very small integrals we can approximate them with the area of the immediate rectangle
</p>


<div class="figure">
<p><object type="image/svg+xml" data="./prob-density.svg" class="org-svg">
Sorry, your browser does not support SVG.</object>
</p>
</div>


<p>
The size of the <i>&delta;x</i> isn't really important, but we can see that given a few points <i>x<sub>0</sub></i>, <i>x<sub>1</sub></i>, <i>x<sub>2</sub></i> .. etc.  we can use their equivalent <b>probability densities</b> <i>f(x<sub>0</sub>)</i>, <i>f(x<sub>1</sub>)</i>,  <i>f(x<sub>2</sub>)</i>, etc.  to say in effect that one was more likely than the other
</p>
</div>
</div>

<div id="outline-container-org43721ef" class="outline-3">
<h3 id="org43721ef">Estimating a PDF</h3>
<div class="outline-text-3" id="text-org43721ef">
<p>
Often in an experimental scenario we will get a series of measurements from which we woujld like to estimate a PDF. There is typically some meta information about the process that generated the values which tells you the general formula for the PDF.
</p>

<p>
We can guess parameters for the PDF and then generate probabilities that the values landed in the vicinity of each measurment. For each measurement, the probability of a measurement being in its vicinity is the area of its equivalent skinny rectangle: <i>f(x<sub>n</sub>)&delta;x</i>. And the cumulative probability of all the measurements landing where they did is just the product of the individual probabilities.
</p>
<blockquote>
<p>
f(x<sub>0</sub>) &delta;x &times; f(x<sub>1</sub>) &delta;x &times; f(x<sub>2</sub>) &delta;x &times; &hellip;
</p>
</blockquote>
<p>
This ofcourse is a tiny value, that gets smaller the smaller the <i>&delta;x</i> and the more measurements you make.
</p>

<p>
However the goal now is to adjust the PDF parameters so that this cumulative probability is maximized. Maximizing this product clearly doesn't depend on the value of <i>&delta;x</i> you've chosen, so we can remove those
</p>
<blockquote>
<p>
f(x<sub>0</sub>) &times; f(x<sub>1</sub>) &times; f(x<sub>2</sub>) &times; &hellip;
</p>
</blockquote>
<p>
As is most often the case, maximization is done by differentiation and setting the result to zero.
</p>
</div>
</div>

<div id="outline-container-orgac9d27a" class="outline-3">
<h3 id="orgac9d27a">Example: Estimating the Normal</h3>
<div class="outline-text-3" id="text-orgac9d27a">
<p>
As an example, we can plug in the previously introduced <i>normal distribution</i> for the PDF <i>f(x)</i> and then assume some measurements <i>x<sub>0</sub></i> <i>x<sub>1</sub></i> <i>x<sub>2</sub></i>  .. 
We know the PDF equation:
</p>
<blockquote>
<p>
f(x) = [1/(&sigma;&radic;2&pi;)]  e<sup>([x-X<sub>center</sub>]<sup>2</sup>)/(2&sigma;<sup>2</sup>)</sup><br>
</p>
</blockquote>
<p>
and so the cumulative probability will be:
</p>
<blockquote>
<p>
f(x<sub>0</sub>)&times;f(x<sub>1</sub>)&times;f(x<sub>2</sub>)&times; .. <br>
[1/(&sigma;&radic;2&pi;)]<sup>n</sup>  e<sup>&sum; ([x<sub>i</sub>-X<sub>center</sub>]<sup>2</sup>)/(2&sigma;<sup>2</sup>)</sup>
</p>
</blockquote>
<p>
<i>X<sub>center</sub></i> and <i>&sigma;</i> are our two unknow parameters. We could now differentiate and solve to find the maximal value
</p>
</div>
</div>
</div>

<div id="outline-container-org7d7559f" class="outline-2">
<h2 id="org7d7559f">Descriptive statistics</h2>
<div class="outline-text-2" id="text-org7d7559f">
<p>
A very unusual effect of continious random variables is that we can often skip their probability distributions entirely and summerizing them with just the <i>mean</i> and <i>variance</i>
</p>
</div>

<div id="outline-container-org31ae7ab" class="outline-4">
<h4 id="org31ae7ab">Expected Value/Mean</h4>
<div class="outline-text-4" id="text-org31ae7ab">
<p>
The expected value of a random variable is the "average" of the probability distribution. Usually written as <b>E[x]</b>. For continious functions this translates to the integral 
</p>
<blockquote>
<p>
E[x] = &int;f(u)&times;u   &hellip; <i>(from  -&infin; to &infin;)</i>
</p>
</blockquote>
<p>
(the notation is a bit confusing b/c <b>x</b> is a random variable and <b>u</b> is a possible value for <b>x</b> and <b>f(u)</b> is a probability density for that value)
</p>
</div>
</div>

<div id="outline-container-org10d4117" class="outline-4">
<h4 id="org10d4117">Variance</h4>
<div class="outline-text-4" id="text-org10d4117">
<p>
The variance is a value to describe how much the random variable differs from its <i>expected value</i>
</p>
<blockquote>
<p>
Var[x]= E[(E[x]-x)<sup>2</sup>]
</p>
</blockquote>
<p>
The power-of-2 ensures that the you are integrating over positive values. If the expected value of the random variable is <b>0</b> then the variance is just <b>E[x<sup>2</sup>]</b> or <i>&int;(f(u))<sup>2</sup></i>
</p>
<dl class="org-dl">
<dt><b>TODO</b></dt><dd>Why not the absolute value? It's not just a mathematical convenience&#x2026; It has some numerical advantage (I just forget what it is)</dd>
</dl>
</div>
</div>

<div id="outline-container-org6a4b6f3" class="outline-4">
<h4 id="org6a4b6f3">Standard deviation</h4>
<div class="outline-text-4" id="text-org6a4b6f3">
<p>
The <i>standard deviation</i> is just the square root of the variance and denoted with <i>&sigma;</i>: 
</p>
<blockquote>
<p>
Var[x] = &sigma;<sub>x</sub><sup>2</sup>
</p>
</blockquote>
</div>
</div>
<div id="outline-container-org25cc05f" class="outline-3">
<h3 id="org25cc05f">Estimating the Normal's descriptive statistics</h3>
<div class="outline-text-3" id="text-org25cc05f">
<p>
The Normal Distributions is a bit unusual in that the mean and standard deviation are parameters of the PDF. So when we differentiate and solve for the PDF that fits our measurements we automatically end up with the best estimates for the mean and variance
</p>
</div>
<div id="outline-container-orgdb2cd25" class="outline-4">
<h4 id="orgdb2cd25">Mean of the Normal</h4>
<div class="outline-text-4" id="text-orgdb2cd25">
<p>
Finishing what we started before, we want to pick a <b>X<sub>center</sub></b> to maximize the cumulative probability of our measurements, which  was
</p>
<blockquote>
<p>
[1/(&sigma;&radic;2&pi;)]<sup>n</sup>  e<sup>&sum; ([x<sub>i</sub>-X<sub>center</sub>]<sup>2</sup>)/(2&sigma;<sup>2</sup>)</sup>
</p>
</blockquote>
<p>
Fortunately, even though the cumulative probability equation  a bit complicated, we only need to maximize the exponent part b/c everything else is not a function of <b>X<sub>center</sub></b>
</p>
<blockquote>
<p>
&sum; (x<sub>i</sub>-X<sub>center</sub>)<sup>2</sup>/(2&sigma;<sup>2</sup>)
</p>
</blockquote>
<p>
To find its maximum we differentiating and set equal to zero:
</p>
<blockquote>
<p>
&sum; (x<sub>i</sub>-X<sub>center</sub>) = 0
</p>
</blockquote>

<p>
Which gives us a 
</p>
<blockquote>
<p>
X<sub>center</sub> = &sum;x<sub>i</sub>/N
</p>
</blockquote>
</div>
</div>

<div id="outline-container-org4caa3d3" class="outline-4">
<h4 id="org4caa3d3">Standard deviation of the normal</h4>
<div class="outline-text-4" id="text-org4caa3d3">
<p>
To estimate an optimal standard deviation <b>&sigma;</b> we follow the procedure as with the mean <b>X<sub>center</sub></b>. We differentiate with respect to <b>&sigma;</b> and then solve for when it's equal to zero. Unlike before, we can't just maximize the exponent part b/c in this case there is a <b>/sigma{}</b> in the factor in front as well as in the exponent. So we need to differentiate <i>by-parts</i>
</p>
<blockquote>
<p>
0 = d/d&sigma;  [1/(&sigma;&radic;2&pi;)]<sup>n</sup>  e<sup>&sum; ([x<sub>i</sub>-X<sub>center</sub>]<sup>2</sup>)/(2&sigma;<sup>2</sup>)</sup><br>
&sigma;<sub>best-estimate</sub> = &radic; (1/N) &sum;(x<sub>i</sub>-X<sub>center</sub>)<sup>2</sup>
</p>
</blockquote>
<p>
The next issue we see is that the solution for <b>&sigma;</b> is a function of <b>X<sub>center</sub></b> which we also don't know. The best we can do here is to subsitute in the best estimate we'd just found in the previous section:  <b>&sum;x<sub>i</sub>/N</b>
</p>

<p>
This unfortunately introducing a <i>bias</i>! <br>
</p>

<p>
The estimated mean is such that the measurments are situated "optimally" around it. The true mean, whatever it is, will be <i>guaranteed</i> to be worse relative to our measurements and so the measurements in actuality deviate more from the mean than from its estimate. The true  <b>X<sub>center</sub></b> of the underlying random variable will not be perfectly centered and the true standard deviation will always be a bit higher. The correct estimate for <b>&sigma;</b> will be:
</p>
<blockquote>
<p>
&sigma;<sub>best-estimate</sub> = &radic; (1/[N-1]) &sum;(x<sub>i</sub>-X<sub>center</sub>)<sup>2</sup>
</p>
</blockquote>
<p>
Proof of this is ommitted.. but do note that as N gets larger this bias becomes insignificant
</p>
</div>
</div>
</div>

<div id="outline-container-org30705d3" class="outline-3">
<h3 id="org30705d3">SigFigs</h3>
<div class="outline-text-3" id="text-org30705d3">
<p>
We can then use these two descriptive statistics to write out a random variable as a <i>value</i> + <i>standard deviation</i> pair. This is often what you will see on an instrument or quoted in a scientific publication. The underlying probability distribution will often either be infered or, as we'll soon see, will be irrelevant
</p>

<p>
Uncertainties/standard-deviation should typically be one digit. And they should be rounded to be on the order of the significant figure.
</p>
<blockquote>
<p>
2.45 &plusmn; 0.04
</p>
</blockquote>
<p>
When we later see how to estimate these values it'll become clear that it doesn't make much sense to have more digits of precision in your uncertainty than in your expected value. In some cases you may want to leave more sigfigs in the uncertainty like if the number is two significant figures and the first digit is small
</p>
<blockquote>
<p>
1.3 &plusmn; 0.4<br>
1.3 &plusmn; 0.37
</p>
</blockquote>
<p>
You can also express the uncertainty as a a fraction or a percentage. Given an uncertainty
</p>
<blockquote>
<p>
x<sub>0</sub> &plusmn; &delta;x<sub>0</sub>
</p>
</blockquote>
<p>
You would write is as
</p>
<blockquote>
<p>
x<sub>0</sub> with a  [100*&delta;x<sub>0</sub>/x<sub>0</sub>]  percent uncertainty<br>
or<br>
x<sub>0</sub> &plusmn; xx%
</p>
</blockquote>

<div class="org-src-container">
<pre class="src src-octave"><span style="color: #a020f0;">function</span> [value<span style="color: #483d8b;">,</span> fractional] <span style="color: #483d8b;">=</span> <span style="color: #0000ff;">uncertainty2fractional</span>(x<span style="color: #483d8b;">,</span> dx)
  value <span style="color: #483d8b;">=</span> x
  fractional <span style="color: #483d8b;">=</span> (dx<span style="color: #483d8b;">/</span>x)
<span style="color: #a020f0;">end</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-octave">[x fx]  <span style="color: #483d8b;">=</span> uncertainty2fractional(50<span style="color: #483d8b;">,</span>25)
ans <span style="color: #483d8b;">=</span> [x fx]
</pre>
</div>

<div class="org-src-container">
<pre class="src src-org"><span style="color: #0000ff;">| 50 | 0.5 |</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-octave"><span style="color: #a020f0;">function</span> [value<span style="color: #483d8b;">,</span> uncertainty] <span style="color: #483d8b;">=</span> <span style="color: #0000ff;">fractional2uncertainty</span>(x<span style="color: #483d8b;">,</span> fx)
  value <span style="color: #483d8b;">=</span> x
  uncertainty <span style="color: #483d8b;">=</span> x<span style="color: #483d8b;">*</span>fx
<span style="color: #a020f0;">end</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-octave">[x dx]  <span style="color: #483d8b;">=</span> fractional2uncertainty(50<span style="color: #483d8b;">,</span>0.5)
ans <span style="color: #483d8b;">=</span> [x dx]
</pre>
</div>

<div class="org-src-container">
<pre class="src src-org"><span style="color: #0000ff;">| 50 | 25 |</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org2f628aa" class="outline-3">
<h3 id="org2f628aa">Adding two random variables</h3>
<div class="outline-text-3" id="text-org2f628aa">
<p>
Here we begin to see the utility of <i>descriptive statistics</i>. They don't just serve as a quick summary of the random variable, but are something that can be manipulated directly.
</p>
</div>
<div id="outline-container-org6f16601" class="outline-4">
<h4 id="org6f16601">Expected Value</h4>
<div class="outline-text-4" id="text-org6f16601">
<p>
If we add two random variables <b>z=x+y</b> then, regardless of their underlying probability distributions, their estimated values will add directly.
</p>
<blockquote>
<p>
<b>E[z] = E[x+y] = E[x] + E[y]</b>
</p>
</blockquote>
<p>
This is a consequence of integrals of sums <i>&int;(f(u)&times;u+g(u)&times;u) = &int;f(u)&times;u+&int;g(u)&times;u</i>
</p>
</div>
</div>

<div id="outline-container-orgf15435c" class="outline-4">
<h4 id="orgf15435c">Variance</h4>
<div class="outline-text-4" id="text-orgf15435c">
<p>
The variance will also add directly, irrespective of distribution, as long as <i>a</i> and <i>b</i> are not correlated. To see this, imagine for simplicity that you have two random variables <b>x</b> and <b>y</b> that have an expected value of <b>0</b>.  We will say the random variable <b>z</b> is their sum <b>z=x+y</b>
</p>

<blockquote>
<p>
E[x] = 0 <br>
Var(x) = E[0 - x<sup>2</sup>] = E[x<sup>2</sup>] <br>
E[y] = 0 <br>
Var(y) = E[0 - y<sup>2</sup>] = E[y<sup>2</sup>] <br>
E[z] = E[x] + E[y] = 0 <br>
Var[z]= E[(E[z]-(z))<sup>2</sup>] = E[(0-(x+y))<sup>2</sup>] = E[(x+y)<sup>2</sup>] = E[x<sup>2</sup>+y<sup>2</sup>+xy] = E[x<sup>2</sup>] + E[y<sup>2</sup>] + E[xy]
</p>
</blockquote>

<p>
As long as <i>a</i> and <i>b</i> are uncorrelated, we can split the last term <b>E[xy]</b> into <b>E[x]*E[y]</b> to see that it is equal to <b>0</b> (you can again go back to the integral definition here). So we are left with the fact that the variance of the sum is the sum of the variances
</p>
<blockquote>
<p>
Var[z]= Var[x] + Var[y]
</p>
</blockquote>
</div>
</div>
<div id="outline-container-org282a577" class="outline-4">
<h4 id="org282a577">Standard Deviation</h4>
<div class="outline-text-4" id="text-org282a577">
<p>
This makes the standard deviation the <i>quadrature sum</i> of the two random variables
</p>

<blockquote>
<p>
&sigma;<sub>z</sub> = &radic;[E[x<sup>2</sup>] + E[y<sup>2</sup>]] = &radic;(&sigma;<sub>x</sub><sup>2</sup> + &sigma;<sub>y</sub><sup>2</sup>)
</p>
</blockquote>
</div>
</div>

<div id="outline-container-org0e47a72" class="outline-4">
<h4 id="org0e47a72">SigFigs</h4>
<div class="outline-text-4" id="text-org0e47a72">
<p>
Combining the two, we can now write out the sum of two random variables as:
</p>

<blockquote>
<p>
(x&plusmn;&delta;x) + (y&plusmn;&delta;y) = q&plusmn;&delta;q<br>
q = x+y<br>
&delta;q = &radic;{&delta;x<sup>2</sup>+&delta;y<sup>2</sup>}
</p>
</blockquote>

<div class="org-src-container">
<pre class="src src-octave"><span style="color: #a020f0;">function</span> [z<span style="color: #483d8b;">,</span> dz] <span style="color: #483d8b;">=</span> <span style="color: #0000ff;">uncertainSum</span>(x<span style="color: #483d8b;">,</span> dx<span style="color: #483d8b;">,</span> y<span style="color: #483d8b;">,</span> dy)
  z <span style="color: #483d8b;">=</span> x <span style="color: #483d8b;">+</span> y
  dz <span style="color: #483d8b;">=</span> sqrt(dx<span style="color: #483d8b;">^</span>2<span style="color: #483d8b;">+</span>dy<span style="color: #483d8b;">^</span>2)
<span style="color: #a020f0;">end</span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-octave">[z dz] <span style="color: #483d8b;">=</span> uncertainSum(10<span style="color: #483d8b;">,</span> 1<span style="color: #483d8b;">,</span> 20<span style="color: #483d8b;">,</span> 3)
ans <span style="color: #483d8b;">=</span> [z dz]
</pre>
</div>

<div class="org-src-container">
<pre class="src src-org"><span style="color: #0000ff;">| 30 | 3.16227766016838 |</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-org3105f5d" class="outline-4">
<h4 id="org3105f5d">Example: Standard deviation of the mean (SDOM)</h4>
<div class="outline-text-4" id="text-org3105f5d">
<p>
Using these new properties we can now calculate how accurate our previous estimate for the mean was. The estimate was to add up the measurements and then divide by the total number of them:
</p>

<blockquote>
<p>
 X<sub>center</sub> = &sum;x<sub>i</sub>/N <br>
= x<sub>0</sub>/N + x<sub>1</sub>/N + x<sub>2</sub>/N + &hellip;
</p>
</blockquote>

<p>
We know each measurement comes from a random variable of mean <b>X<sub>center</sub></b> and standard deviation <b>&sigma;<sub>x</sub></b> so we can write it out using our previous notation
</p>
<blockquote>
<p>
= (X<sub>center</sub>&plusmn;&sigma;<sub>X</sub>)/N + (X<sub>center</sub>&plusmn;&sigma;<sub>X</sub>)/N + (X<sub>center</sub>&plusmn;&sigma;<sub>X</sub>)/N + &hellip; <br>
= (X<sub>center</sub>/N&plusmn;&sigma;<sub>X</sub>/N) + (X<sub>center</sub>/N&plusmn;&sigma;<sub>X</sub>/N) + (X<sub>center</sub>/N&plusmn;&sigma;<sub>X</sub>/N) + &hellip;
</p>
</blockquote>

<p>
So now we just need to do a quadrature sum of these deviations to get a total deviation:
</p>

<blockquote>
<p>
&sigma;<sub>X<sub>center</sub></sub> = &radic; &sum; [ (&sigma;<sub>x</sub>/N)<sup>2</sup> + (&sigma;<sub>x</sub>/N)<sup>2</sup> + (&sigma;<sub>x</sub>/N)<sup>2</sup> + &hellip; ] <br>
= &radic;[N &times; (&sigma;<sub>x</sub>/N)<sup>2</sup> ] <br>
= &radic;[&sigma;<sub>x</sub><sup>2</sup>/N ] <br>
= &sigma;<sub>x</sub>/&radic;N
</p>
</blockquote>
<p>
In some situations this can provide a useful error. Particularly if you have a noisy method of measurement but the thing you are measuring is pretty constant. For instance if you're measuring a resistor. The resistor is of a constant value and doesn't fluctuate, but your voltmeter/power source will have some error
</p>
<blockquote>
<p>
<b>Side note..</b> : the uncertainty in <b>&sigma;<sub>x</sub></b> is: <b>1 /&radic;[2(N-1)] <br>
</b>
<i>Proof omitted for now..</i>
</p>
</blockquote>
</div>
</div>
</div>
<div id="outline-container-orgdd250d7" class="outline-3">
<h3 id="orgdd250d7">Multiplying two random variables</h3>
<div class="outline-text-3" id="text-orgdd250d7">
<p>
Again, we work directly with the descriptive statistics without even looking at the variables' PDFs
</p>
</div>
<div id="outline-container-org0c072b0" class="outline-4">
<h4 id="org0c072b0">Expected Value</h4>
<div class="outline-text-4" id="text-org0c072b0">
<blockquote>
<p>
E[z] = E[x] &times; E[y]
</p>
</blockquote>
</div>
</div>
<div id="outline-container-orgb05d64a" class="outline-4">
<h4 id="orgb05d64a">Variance</h4>
<div class="outline-text-4" id="text-orgb05d64a">
<p>
We start directly with directly applying the equation for the variance
</p>
<blockquote>
<p>
Var(xy) = E[(xy-XY)<sup>2</sup>] &hellip;
</p>
</blockquote>
<p>
<b>X</b> and <b>Y</b> are short hand for <b>E[x]</b> and <b>E[y]</b>
</p>

<p>
First we focus on massaging the inner term <b>xy-XY</b>
</p>
<blockquote>
<p>
XY &times; [x/X &times; y/Y -1] <br>
XY &times; [ ([x-X]/X + 1) &times; ([y-Y]/Y + 1)  - 1 ] <br>
XY &times; [ [x-X]/X + [y-Y]/Y + [x-X]&times;[y-Y]/XY ]
</p>
</blockquote>
<p>
if <b>&delta;x = [x-X]/X</b> and <b>&delta;y = [y - Y]/Y</b>, then we can rewrite this as
</p>
<blockquote>
<p>
XY &times; [&delta;x+&delta;y+ &delta;x&delta;y]
</p>
</blockquote>
<p>
So now we can rewrite the original equation
</p>
<blockquote>
<p>
Var(xy) = E[(xy-XY)<sup>2</sup>] <br>
= (XY)<sup>2</sup> &times; E[(&delta;x + &delta;y + &delta;x&delta;y)<sup>2</sup>] <br>
= (XY)<sup>2</sup> &times; E[&delta;x<sup>2</sup> + &delta;x&delta;y + &delta;x<sup>2</sup>&delta;y + &delta;y&delta;x + &delta;y<sup>2</sup> + &delta;x&delta;y<sup>2</sup> + &delta;x<sup>2</sup>&delta;y + &delta;x&delta;y<sup>2</sup> + &delta;x<sup>2</sup>&delta;y<sup>2</sup>] <br>
= (XY)<sup>2</sup> &times; (E[&delta;x<sup>2</sup>] + E[&delta;x&delta;y] + E[&delta;x<sup>2</sup>&delta;y] + E[&delta;y&delta;x] + E[&delta;y<sup>2</sup>] + E[&delta;x&delta;y<sup>2</sup>] + E[&delta;x<sup>2</sup>&delta;y] + E[&delta;x&delta;y<sup>2</sup>] + E[&delta;x<sup>2</sup>&delta;y<sup>2</sup>]) <br>
</p>
</blockquote>
<p>
Not mentioned explicitely till now, but when we have want to find the <i>expected value</i> of a equation of multiple random variables  we need to integrate across all variables. So in this case we want to integrate across <b>x</b> and <b>y</b> from -&infin; to &infin;. As premised at the beginning, <b>x</b> and <b>y</b> are independent, so when we write out these expected values we can actually rearrange them as each integral is independent (the other terms is treated as a constant in each integral). 
</p>

<blockquote>
<p>
E[x<sup>n</sup>y<sup>m</sup>] = &int;<sub>x</sub>&int;<sub>y</sub>x<sup>n</sup>y<sup>m</sup><br>
E[x<sup>n</sup>y<sup>m</sup>] = &int;<sub>x</sub>x<sup>n</sup>&int;<sub>y</sub>y<sup>m</sup>
</p>
</blockquote>
<p>
The final trick is to notice that <b>&delta;x</b> and <b>&delta;y</b> have an expected values are <i>zero</i> - so if either <b>m</b> or <b>n</b> are equal to <b>1</b> then it's integral goes to zero and so <b>E[x<sup>n</sup>y<sup>m</sup>]</b> comes out to <b>0</b>. The only nonzero terms that remain are
</p>
<blockquote>
<p>
= (XY)<sup>2</sup> &times; (E[&delta;x<sup>2</sup>] + E[&delta;y<sup>2</sup>] + E[(&delta;x&delta;y)<sup>2</sup>]) <br>
</p>
</blockquote>
<blockquote>
<p>
= (XY)<sup>2</sup> &times; (V[x]/X<sup>2</sup> + V[y]/Y<sup>2</sup> + V[x]V[y]/(XY)<sup>2</sup>) <br>
= Y<sup>2</sup>V[x] + X<sup>2</sup>V[y] + V[x]V[y] <br>
</p>
</blockquote>
</div>
</div>
<div id="outline-container-orgf1428ec" class="outline-4">
<h4 id="orgf1428ec">Standard Deviation</h4>
<div class="outline-text-4" id="text-orgf1428ec">
<p>
The standard deviation is the square root of the variance.. 
</p>
<blockquote>
<p>
&sigma;<sub>xy</sub> &cong; &radic; [Y<sup>2</sup>V[x] + X<sup>2</sup>V[y] + V[x]V[y]]
</p>
</blockquote>
</div>
</div>
<div id="outline-container-orgd099f51" class="outline-4">
<h4 id="orgd099f51">SigFigs</h4>
<div class="outline-text-4" id="text-orgd099f51">
<p>
Again, summarizing in our standard notation we get
</p>
<blockquote>
<p>
(x&plusmn;&delta;x) &times; (y&plusmn;&delta;y) = q&plusmn;&delta;q<br>
q = x&times;y<br>
&delta;q/q = &radic;[x<sup>2</sup>&delta;y<sup>2</sup> + y<sup>2</sup>&delta;x<sup>2</sup> + &delta;x<sup>2</sup>&delta;y<sup>2</sup>]
</p>
</blockquote>

<div class="org-src-container">
<pre class="src src-octave"><span style="color: #a020f0;">function</span> [z<span style="color: #483d8b;">,</span> dz] <span style="color: #483d8b;">=</span> <span style="color: #0000ff;">uncertainProduct</span>(x<span style="color: #483d8b;">,</span> dx<span style="color: #483d8b;">,</span> y<span style="color: #483d8b;">,</span> dy)
  z <span style="color: #483d8b;">=</span> x<span style="color: #483d8b;">*</span>y
  dz <span style="color: #483d8b;">=</span> sqrt(x<span style="color: #483d8b;">^</span>2<span style="color: #483d8b;">*</span>dy<span style="color: #483d8b;">^</span>2 <span style="color: #483d8b;">+</span> y<span style="color: #483d8b;">^</span>2<span style="color: #483d8b;">*</span>dx<span style="color: #483d8b;">^</span>2 <span style="color: #483d8b;">+</span> dx<span style="color: #483d8b;">^</span>2<span style="color: #483d8b;">*</span>dy<span style="color: #483d8b;">^</span>2)
<span style="color: #a020f0;">end</span>
</pre>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-orgc3eb256" class="outline-2">
<h2 id="orgc3eb256">Other Topics</h2>
<div class="outline-text-2" id="text-orgc3eb256">
</div>
<div id="outline-container-org21f9090" class="outline-3">
<h3 id="org21f9090">Data Rejection</h3>
<div class="outline-text-3" id="text-org21f9090">
<p>
Sometimes things go wrong and data is just way out there
</p>
</div>
<div id="outline-container-org433077b" class="outline-4">
<h4 id="org433077b">Chauvenet's Criterion</h4>
<div class="outline-text-4" id="text-org433077b">
<p>
This is a scheme for rejecting data - but it only makes sense if your data is normally distributed
You first find how many standard deviation this measurement is away from your estimated mean
</p>
<blockquote>
<p>
t<sub>sus</sub> = |x<sub>sus</sub> - X<sub>center</sub>|/&sigma;<sub>x</sub>
</p>
</blockquote>
<p>
Then you look up the probability a measurement is that far off in a table. This probability represents the fraction of points that will lie either at the point in question or further out. Multiply it times the amount of measurements you've done, <b>N</b>,  and you get the amount of points you'd expect to lie at the point-or-further after <b>N</b> measurements.
</p>
<blockquote>
<p>
N &times; Prob(outside t<sub>sus</sub>&sigma;)
</p>
</blockquote>
<p>
If the value is smaller than <b>1/2</b> then we expect less tan half a point out as far as the point in question - so you can reject the point and recalculate your mean/sigma.
</p>
</div>
</div>
</div>
<div id="outline-container-org0a3c2f0" class="outline-3">
<h3 id="org0a3c2f0">Weighted Average</h3>
<div class="outline-text-3" id="text-org0a3c2f0">
<p>
We often want to average different measurements into one. It's clear a measurement with a larger error should have a smaller effect on the outcome than a measurement with a smaller error. 
</p>
</div>
<div id="outline-container-org41b101c" class="outline-4">
<h4 id="org41b101c">Mean</h4>
<div class="outline-text-4" id="text-org41b101c">
<p>
To get the combined mean we proceed in a direct manner. We multiply the two PDFs and minimize the result
</p>
<blockquote>
<p>
pdf(x<sub>a</sub>) = (1/(&sigma;<sub>a</sub>&radic;2&pi;))  e<sup>(x<sub>a</sub>-X<sub>center</sub>)<sup>2</sup>/(2&sigma;<sub>a</sub><sup>2</sup>)</sup><br>
pdf(x<sub>b</sub>) = (1/(&sigma;<sub>b</sub>&radic;2&pi;))  e<sup>(x<sub>b</sub>-X<sub>center</sub>)<sup>2</sup>/(2&sigma;<sub>b</sub><sup>2</sup>)</sup><br>
</p>
</blockquote>
<p>
Here <b>x<sub>a</sub></b> and <b>x<sub>b</sub></b> are the two prior measurements/estimates we have, and the <b>X<sub>center</sub></b> will be our combined average mean. We need to choose an  <b>X<sub>center</sub></b> that will maximize the probability of both functions. We combine probabilities by multiplication
</p>
<blockquote>
<p>
pdf(x<sub>a</sub>) &times; pdf(x<sub>b</sub>) <br>
(1/(&sigma;<sub>a</sub>&radic;2&pi;))  e<sup>(x<sub>a</sub>-X<sub>center</sub>)<sup>2</sup>/(2&sigma;<sub>a</sub><sup>2</sup>)</sup> &times; (1/(&sigma;<sub>b</sub>&radic;2&pi;))  e<sup>(x<sub>b</sub>-X<sub>center</sub>)<sup>2</sup>/(2&sigma;<sub>b</sub><sup>2</sup>)</sup> <br>
(1/(&sigma;<sub>a</sub>&sigma;<sub>b</sub>2&pi;)) e<sup>(x<sub>a</sub>-X<sub>center</sub>)<sup>2</sup>/(2&sigma;<sub>a</sub><sup>2</sup>) + (x<sub>b</sub>-X<sub>center</sub>)<sup>2</sup>/(2&sigma;<sub>b</sub><sup>2</sup>)</sup> <br>
</p>
</blockquote>
<p>
Again here we just care to maximize the exponent (the only part that's a function of <b>X<sub>center</sub></b>). We take its derivative and set to zero
</p>
<blockquote>
<p>
0 = d/dX (x<sub>a</sub>-X<sub>center</sub>)<sup>2</sup>/(2&sigma;<sub>a</sub><sup>2</sup>) + (x<sub>b</sub>-X<sub>center</sub>)<sup>2</sup>/(2&sigma;<sub>b</sub><sup>2</sup>)<br>
0 = (x<sub>a</sub>-X<sub>center</sub>)/&sigma;<sub>a</sub><sup>2</sup> + (x<sub>b</sub>-X<sub>center</sub>)/&sigma;<sub>b</sub><sup>2</sup> <br>
X<sub>center</sub> = [x<sub>a</sub>/&sigma;<sub>a</sub><sup>2</sup> + x<sub>b</sub>/&sigma;<sub>b</sub><sup>2</sup>] / [(1/&sigma;<sub>a</sub><sup>2</sup> + 1/&sigma;<sub>b</sub><sup>2</sup>]
</p>
</blockquote>
<p>
You can also rewrite this by introducing a new variable called a <b>weight</b>
</p>
<blockquote>
<p>
w = 1/&sigma;<sup>2</sup>
</p>
</blockquote>
</div>
</div>
<div id="outline-container-orgb169a8e" class="outline-4">
<h4 id="orgb169a8e">Standard Deviation</h4>
<div class="outline-text-4" id="text-orgb169a8e">
<p>
The standard deviation we derive from the equation for the mean by applying error propagation.
The simplifying trick is to first look at the variance of the sum: <b>Var[z]= Var[x] + Var[y]</b>. 
If you immediately try to calculate standard deviation and use the quadrature rule then things get overly complicated
(as a general rule, it's easier to deal with variances b/c you avoid the quadrature mathematical mess)
</p>
<blockquote>
<p>
X<sub>center</sub> = [x<sub>a</sub>/&sigma;<sub>a</sub><sup>2</sup> + x<sub>b</sub>/&sigma;<sub>b</sub><sup>2</sup>] / [(1/&sigma;<sub>a</sub><sup>2</sup> + 1/&sigma;<sub>b</sub><sup>2</sup>] <br>
Var[center] = Var[x<sub>a</sub>]/[&sigma;<sub>a</sub><sup>2</sup> + Var[x<sub>b</sub>]/[&sigma;<sub>b</sub><sup>2</sup> / [(1/&sigma;<sub>a</sub><sup>2</sup> + 1/&sigma;<sub>b</sub><sup>2</sup>]
</p>
</blockquote>
<p>
Since <b>Var[w] = &sigma;<sub>w</sub><sup>2</sup></b>, the top terms all cancel out and you're left with the bottom term only
</p>
<blockquote>
<p>
Var[center] = 1 / [(1/&sigma;<sub>a</sub><sup>2</sup> + 1/&sigma;<sub>b</sub><sup>2</sup>] <br>
&sigma;<sub>center</sub> = 1 / &radic;[(1/&sigma;<sub>a</sub><sup>2</sup> + 1/&sigma;<sub>b</sub><sup>2</sup>]
</p>
</blockquote>
</div>
</div>
</div>
</div>
<div id="outline-container-org8baf549" class="outline-2">
<h2 id="org8baf549">Multiple Variables</h2>
<div class="outline-text-2" id="text-org8baf549">
</div>
<div id="outline-container-org48f8e88" class="outline-3">
<h3 id="org48f8e88">Functions of two variables</h3>
<div class="outline-text-3" id="text-org48f8e88">
</div>
<div id="outline-container-orge74a6c5" class="outline-4">
<h4 id="orge74a6c5">Mean</h4>
<div class="outline-text-4" id="text-orge74a6c5">
<p>
Often we are left in a situation where we have a function of multiple variables. For simplicity we look at the simple case of a function of two variable <b>z=q(x,y)</b>. We can make observations of <b>x</b> and <b>y</b> and calculate corresponding <b>z</b> values. In this way we can then estimate the mean of <b>z</b>.
</p>
<blockquote>
<p>
z<sub>i</sub>=q(x<sub>i</sub>,y<sub>i</sub>) <br>
z<sub>mean</sub> = 1/N &sum; z<sub>i</sub> <br>
z<sub>mean</sub> = q(x<sub>i</sub>,y<sub>i</sub>)
</p>
</blockquote>
<p>
This solution is workable, however we can also construct an alternative by using a first order approximation if we a priori have an estimate for <b>x</b> and <b>y</b>. Notice that <b>q(x,y)</b> is some surface and if we differentiate at <b>x<sub>mean</sub></b> and <b>y<sub>mean</sub></b> we can get an estimate for the local slope. Then each measurement <b>x<sub>i</sub></b>, <b>y<sub>i</sub></b> will have a direct linear effect on <b>q<sub>i</sub></b>
</p>
<blockquote>
<p>
z<sub>i</sub> = q(x<sub>mean</sub>,y<sub>mean</sub>) + &part;q/&part;x (x<sub>i</sub> - x<sub>mean</sub>) + &part;q/&part;y (y<sub>i</sub> - y<sub>mean</sub>)
</p>
</blockquote>
<p>
And now the average will be the sum of these approximations
</p>
<blockquote>
<p>
z<sub>mean</sub> = 1/N &sum; z<sub>i</sub> <br>
z<sub>mean</sub> = 1/N &sum; q(x<sub>mean</sub>,y<sub>mean</sub>) + &part;q/&part;x (x<sub>i</sub> - x<sub>mean</sub>) + &part;q/&part;y (y<sub>i</sub> - y<sub>mean</sub>)
</p>
</blockquote>
<p>
The slope terms add up to zero and we are left with 
</p>
<blockquote>
<p>
z<sub>mean</sub> = 1/N &sum; q(x<sub>mean</sub>,y<sub>mean</sub>) <br>
z<sub>mean</sub> = q(x<sub>mean</sub>,y<sub>mean</sub>) 
</p>
</blockquote>
<p>
Which makes life much easier. This should hold true as long as the slopes are good approximations (ie, the variance is relatively small)
</p>
</div>
</div>
<div id="outline-container-orgfb0891e" class="outline-4">
<h4 id="orgfb0891e">Variance</h4>
<div class="outline-text-4" id="text-orgfb0891e">
<p>
We can continue similarly for the variance
</p>
<blockquote>
<p>
Var(z) = 1/N &sum; ( z<sub>i</sub> - z<sub>mean</sub> )<sup>2</sup> <br>
Var(z) = 1/N &sum; ( &part;q/&part;x (x<sub>i</sub> - x<sub>mean</sub>) + &part;q/&part;y (y<sub>i</sub> - y<sub>mean</sub>) )<sup>2</sup> <br>
Var(z) = (&part;q/&part;x)<sup>2</sup>  1/N &sum; (x<sub>i</sub> - x<sub>mean</sub>)<sup>2</sup> + (&part;q/&part;y)<sup>2</sup> 1/N &sum; (y<sub>i</sub> - y<sub>mean</sub>)<sup>2</sup> + 2&times;(&part;q/&part;x)(&part;q/&part;y) 1/N &sum; (x<sub>i</sub> - x<sub>mean</sub>)(y<sub>i</sub> - y<sub>mean</sub>)
</p>
</blockquote>
<p>
Which then simplifies to 
</p>
<blockquote>
<p>
Var(z) = (&part;q/&part;x)<sup>2</sup>  &sigma;<sub>x</sub><sup>2</sup> + (&part;q/&part;y)<sup>2</sup> &sigma;<sub>x</sub><sup>2</sup> + 2&times;(&part;q/&part;x)(&part;q/&part;y) 1/N &sum; (x<sub>i</sub> - x<sub>mean</sub>)(y<sub>i</sub> - y<sub>mean</sub>)
</p>
</blockquote>
<p>
The last term is the <b>covariance</b> written as <b>&sigma;<sub>xy</sub></b>
</p>
<blockquote>
<p>
&sigma;<sub>xy</sub> =  1/N &sum; (x<sub>i</sub> - x<sub>mean</sub>)(y<sub>i</sub> - y<sub>mean</sub>)
</p>
</blockquote>
<p>
Which shortens the complete equation for the variance
</p>
<blockquote>
<p>
Var(z) = (&part;q/&part;x)<sup>2</sup>  &sigma;<sub>x</sub><sup>2</sup> + (&part;q/&part;y)<sup>2</sup> &sigma;<sub>x</sub><sup>2</sup> + 2&times;(&part;q/&part;x)(&part;q/&part;y) &sigma;<sub>xy</sub>
</p>
</blockquote>
<p>
(Reminder: The partial derivatives are evaluated at <b>x<sub>mean</sub></b> and <b>y<sub>mean</sub></b> and correspond to the local slopes in the <b>x</b> and <b>y</b> directions)
</p>

<p>
If the variables are independent then the <b>covariance</b> will be around zero b/c both terms in the multiplication, <b>(x<sub>i</sub> - x<sub>mean</sub>)</b> and <b>(y<sub>i</sub> - y<sub>mean</sub>)</b> are bouncing around zero randomly. If the terms are not independent and when one variable varies from the mean then the other does as well - then this term will be non zero
</p>
</div>
</div>
</div>
<div id="outline-container-org0931e6f" class="outline-3">
<h3 id="org0931e6f">Correlation Coefficient</h3>
<div class="outline-text-3" id="text-org0931e6f">
<p>
To get a more objective value for how much our values seem to be independent we can calculate a sort of "normalized" covariance by dividing by the magnitudes of our variations. This values is called the <b>correlation coefficient</b>, it'd denoted by the letter <b>r</b> and will always lie between <b>-1</b> and <b>+1</b>
</p>
<blockquote>
<p>
r = &sigma;<sub>xy</sub> / &sigma;<sub>x</sub>&sigma;<sub>y</sub> <br>
r = &sum; (x<sub>i</sub> - x<sub>mean</sub>)(y<sub>i</sub> - y<sub>mean</sub>) / &radic; &sum; (x<sub>i</sub> - x<sub>mean</sub>)<sup>2</sup> &sum; (y<sub>i</sub> - y<sub>mean</sub>)<sup>2</sup>
</p>
</blockquote>
<p>
However this value is very sensitive to the number of measurements. If our <b>N</b> is small then this value can be wildly wrong. We need an additional confidence calculation. This is done by using a look up table to see <b>Prob( ||r|| &gt; <i>value</i> | given N measurements)</b>
</p>
</div>
</div>
</div>




<div id="outline-container-org8a35618" class="outline-2">
<h2 id="org8a35618">End</h2>
</div>
</div>
</body>
</html>
